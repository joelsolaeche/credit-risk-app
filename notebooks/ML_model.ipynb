{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to train different ML models and get the best one based on the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "\n",
    "#Defining BASE_PATH\n",
    "BASE_PATH = os.getenv(\"/Users/carlos/Desktop/CURSOS/Anyone AI/Credit-risk-analysis/dataset/\", \"/Users/carlos/Desktop/CURSOS/Anyone AI/Credit-risk-analysis/dataset/\")\n",
    "\n",
    "train_file_path = os.path.join(BASE_PATH, \"X_train_data.csv\")\n",
    "y_train_file_path = os.path.join(BASE_PATH, \"y_train_data.csv\")\n",
    "val_file_path = os.path.join(BASE_PATH, \"X_val_data.csv\")\n",
    "y_val_path = os.path.join(BASE_PATH, \"y_val_data.csv\")\n",
    "\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "\n",
    "y_train_df = pd.read_csv(y_train_file_path)\n",
    "y_train = y_train_df['TARGET_LABEL_BAD']\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "val_df = pd.read_csv(val_file_path)\n",
    "\n",
    "y_valid_df = pd.read_csv(y_val_path)\n",
    "y_valid = y_valid_df['TARGET_LABEL_BAD']\n",
    "y_valid = y_valid.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [12:35:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10377, number of negative: 29623\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1679\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 89\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259425 -> initscore=-1.048959\n",
      "[LightGBM] [Info] Start training from score -1.048959\n",
      "                 Model  Accuracy  Precision    Recall  F1-score   ROC-AUC\n",
      "4              XGBoost    0.7293   0.466872  0.113739  0.182916  0.639892\n",
      "3   MLP Neural Network    0.7017   0.319774  0.106231  0.159482  0.587311\n",
      "5             LightGBM    0.7354   0.561644  0.030781  0.058363  0.646520\n",
      "1        Random Forest    0.7317   0.440252  0.026276  0.049593  0.630287\n",
      "0  Logistic Regression    0.7330   0.416667  0.005631  0.011111  0.610479\n",
      "2                  SVM    0.7335   0.000000  0.000000  0.000000  0.536479\n"
     ]
    }
   ],
   "source": [
    "#Training a set of common ML models used for binary classification looking for the best option\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(solver='liblinear'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(probability=True, kernel='rbf'),\n",
    "    \"MLP Neural Network\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def evaluate_models(models, X_train, y_train, X_val, y_val):\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_proba = model.predict_proba(X_val)[:, 1]  # Probability for class 1 (BAD)\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        roc_auc = roc_auc_score(y_val, y_proba)\n",
    "        \n",
    "        # Store results\n",
    "        results.append([name, acc, precision, recall, f1, roc_auc])\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"ROC-AUC\"])\n",
    "    return results_df\n",
    "\n",
    "# Evaluate models\n",
    "results_df = evaluate_models(models, train_df, y_train, val_df, y_valid)\n",
    "\n",
    "# Display results sorted by F1-score or ROC-AUC\n",
    "print(results_df.sort_values(by=\"F1-score\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost incluye su propia clase para almacenar conjuntos de datos, llamada DMatrix.  \n",
    "### Es una clase muy optimizada en cuanto a memoria y velocidad. \n",
    "### Por eso, convertir conjuntos de datos a este formato es un requisito para la API nativa de XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training xgboost model using scale_pos_weight strategy to compensate the imbalanced dataset.\n",
    "\n",
    "# Convert training and validation sets into XGBoost DMatrix\n",
    "dtrain = xgb.DMatrix(train_df, label=y_train)\n",
    "dvalid = xgb.DMatrix(val_df, label=y_valid)\n",
    "\n",
    "# Define XGBoost Parameters\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",  # Binary classification\n",
    "    \"eval_metric\": \"logloss\",        # Log loss is a good metric for classification\n",
    "    \"eta\": 0.1,                      # Learning rate\n",
    "    \"max_depth\": 6,                   # Tree depth\n",
    "    \"subsample\": 0.8,                 # Sample ratio for training data\n",
    "    \"colsample_bytree\": 0.8,          # Feature selection ratio\n",
    "    \"scale_pos_weight\": np.sum(y_train == 0) / np.sum(y_train == 1),  # Adjust for class imbalance\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# Train the model with early stopping\n",
    "evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "model = xgb.train(params, dtrain, num_boost_round=500, evals=evals, early_stopping_rounds=20, verbose_eval=50)\n",
    "\n",
    "# Predictions\n",
    "y_pred = (model.predict(dvalid) > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "y_proba = model.predict(dvalid)  # Raw probabilities\n",
    "\n",
    "# Compute Evaluation Metrics\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "f1 = f1_score(y_valid, y_pred)\n",
    "roc_auc = roc_auc_score(y_valid, y_proba)\n",
    "report = classification_report(y_valid, y_pred)\n",
    "conf_matrix = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "# Display Results\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Feature Importance using Gain\n",
    "feature_importance = model.get_score(importance_type=\"gain\")\n",
    "sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nFeature Importance (by Gain):\")\n",
    "for feature, importance in sorted_importance:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training xgboost model and getting its metrics (no scale_pos_weight strategy)\n",
    "#In this case xgboost library it's used instead scikit lear wrapper.\n",
    "#It needs to be decided if this is the best choice\n",
    "\n",
    "# Convert datasets to DMatrix (optimized for XGBoost)\n",
    "dtrain = xgb.DMatrix(train_df, label=y_train)\n",
    "dval = xgb.DMatrix(val_df, label=y_valid)\n",
    "\n",
    "# Define model parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # Binary classification\n",
    "    'eval_metric': 'logloss',  # Logarithmic loss\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Train model\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dval, \"Validation\")], early_stopping_rounds=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert test dataset to DMatrix\n",
    "dtest = xgb.DMatrix(val_df)\n",
    "\n",
    "# Make predictions (probabilities)\n",
    "y_pred_proba = xgb_model.predict(dtest)\n",
    "\n",
    "# Convert probabilities to binary predictions (threshold = 0.5)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "f1 = f1_score(y_valid, y_pred)\n",
    "roc_auc = roc_auc_score(y_valid, y_pred)\n",
    "conf_matrix = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_valid, y_pred))\n",
    "\n",
    "\n",
    "# Get feature importance using \"gain\"\n",
    "importance = xgb_model.get_score(importance_type='gain')\n",
    "\n",
    "# Sort by importance\n",
    "sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print feature importance\n",
    "print(\"\\nFeature Importance (Gain):\")\n",
    "for feature, score in sorted_importance:\n",
    "    print(f\"{feature}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's train different models based on the above metrics\n",
    "\n",
    "### Taking into account that the dataset is imbalanced, we'll start trying with next models: XGBoost, LightGBM, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Step 1: Train a basic XGBoost model with default settings\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(train_df, y_train)\n",
    "\n",
    "# Step 2: Evaluate initial performance on validation set\n",
    "y_pred = xgb_model.predict(val_df)\n",
    "y_pred_prob = xgb_model.predict_proba(val_df)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_valid, y_pred_prob))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "\n",
    "# Step 3: Hyperparameter tuning with cross-validation and early stopping\n",
    "\n",
    "# Compute class weights to adjust scale_pos_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=y_train)\n",
    "scale_pos_weight = class_weights[1] / class_weights[0]  # Calculate scale_pos_weight\n",
    "\n",
    "# Set up XGBoost model with hyperparameter tuning\n",
    "params = {\n",
    "    'scale_pos_weight': [scale_pos_weight],  # Use the computed scale_pos_weight\n",
    "    'max_depth': [3, 5, 7],  # Example of tuning max_depth\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Learning rate\n",
    "    'n_estimators': [100, 200],  # Number of estimators\n",
    "    'subsample': [0.8, 1.0],  # Fraction of samples used for each tree\n",
    "    'colsample_bytree': [0.8, 1.0]  # Fraction of features used for each tree\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb.XGBClassifier(eval_metric='mlogloss', random_state=42),\n",
    "    param_grid=params,\n",
    "    scoring=make_scorer(roc_auc_score),  # Scoring by ROC AUC\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model with cross-validation and grid search\n",
    "grid_search.fit(train_df, y_train)\n",
    "\n",
    "# Best parameters and best score from the grid search\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best ROC-AUC score:\", grid_search.best_score_)\n",
    "\n",
    "# Step 4: Evaluate best model on validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_best = best_model.predict(val_df)\n",
    "y_pred_prob_best = best_model.predict_proba(val_df)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Classification Report for Best Model:\")\n",
    "print(classification_report(y_valid, y_pred_best))\n",
    "print(\"ROC-AUC Score for Best Model:\", roc_auc_score(y_valid, y_pred_prob_best))\n",
    "print(\"Confusion Matrix for Best Model:\")\n",
    "print(confusion_matrix(y_valid, y_pred_best))\n",
    "\n",
    "# Step 5: Feature Importance using Gain\n",
    "importance = best_model.get_booster().get_score(importance_type=\"gain\")\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "importance_df = pd.DataFrame(importance.items(), columns=[\"Feature\", \"Gain\"])\n",
    "importance_df = importance_df.sort_values(by=\"Gain\", ascending=False)\n",
    "\n",
    "# Print feature importance\n",
    "print(\"\\nFeature Importance (Gain):\")\n",
    "print(importance_df)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Gain\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Gain\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance (Gain) in XGBoost\")\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Step 1: Compute class weights\n",
    "classes = np.array([0, 1])\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "scale_pos_weight = class_weights[1] / class_weights[0]  # Calculate scale_pos_weight\n",
    "\n",
    "print(\"Scale Pos Weight:\", scale_pos_weight)\n",
    "\n",
    "# Step 2: Train a basic LightGBM model with default settings\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    class_weight='balanced', \n",
    "    scale_pos_weight=scale_pos_weight,  # Adjusting class weights\n",
    "    boosting_type='gbdt',  # Gradient Boosting Decision Tree\n",
    "    objective='binary',  # Binary classification objective\n",
    "    metric='binary_error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "lgb_model.fit(train_df, y_train)\n",
    "\n",
    "# Step 3: Evaluate initial performance on validation set\n",
    "y_pred = lgb_model.predict(val_df)\n",
    "y_pred_prob = lgb_model.predict_proba(val_df)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_valid, y_pred_prob))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "\n",
    "# Step 4: Hyperparameter tuning with cross-validation\n",
    "\n",
    "# Set up the parameter grid for tuning\n",
    "params = {\n",
    "    'class_weight': ['balanced', None],  # Balance class weights automatically or leave it unbalanced\n",
    "    'scale_pos_weight': [scale_pos_weight],  # Adjust scale_pos_weight\n",
    "    'max_depth': [3, 5, 7],  # Example of tuning max_depth\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Learning rate\n",
    "    'n_estimators': [100, 200],  # Number of estimators\n",
    "    'subsample': [0.8, 1.0],  # Fraction of samples used for each tree\n",
    "    'colsample_bytree': [0.8, 1.0]  # Fraction of features used for each tree\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb.LGBMClassifier(objective='binary', metric='binary_error', random_state=42),\n",
    "    param_grid=params,\n",
    "    scoring=make_scorer(roc_auc_score),  # Scoring by ROC AUC\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model with cross-validation and grid search\n",
    "grid_search.fit(train_df, y_train)\n",
    "\n",
    "# Best parameters and best score from the grid search\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best ROC-AUC score:\", grid_search.best_score_)\n",
    "\n",
    "# Step 5: Evaluate the best model on validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_best = best_model.predict(val_df)\n",
    "y_pred_prob_best = best_model.predict_proba(val_df)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Classification Report for Best Model:\")\n",
    "print(classification_report(y_valid, y_pred_best))\n",
    "print(\"ROC-AUC Score for Best Model:\", roc_auc_score(y_valid, y_pred_prob_best))\n",
    "print(\"Confusion Matrix for Best Model:\")\n",
    "print(confusion_matrix(y_valid, y_pred_best))\n",
    "\n",
    "# Step 6: Feature Importance using Gain\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": train_df.columns,\n",
    "    \"Gain\": best_model.booster_.feature_importance(importance_type=\"gain\")\n",
    "})\n",
    "\n",
    "# Sort by Gain\n",
    "importance_df = importance_df.sort_values(by=\"Gain\", ascending=False)\n",
    "\n",
    "# Print feature importance\n",
    "print(\"\\nFeature Importance (Gain):\")\n",
    "print(importance_df)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Gain\"], color=\"lightblue\")\n",
    "plt.xlabel(\"Gain\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance (Gain) in LightGBM\")\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Step 1: Train a basic LightGBM model WITHOUT scale_pos_weight\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    class_weight=None,  # No class weight balancing\n",
    "    boosting_type='gbdt',  # Gradient Boosting Decision Tree\n",
    "    objective='binary',  # Binary classification objective\n",
    "    metric='binary_error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "lgb_model.fit(train_df, y_train)\n",
    "\n",
    "# Step 2: Evaluate initial performance on validation set\n",
    "y_pred = lgb_model.predict(val_df)\n",
    "y_pred_prob = lgb_model.predict_proba(val_df)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_valid, y_pred_prob))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "\n",
    "# Step 3: Hyperparameter tuning with cross-validation\n",
    "\n",
    "# Set up the parameter grid for tuning\n",
    "params = {\n",
    "    'class_weight': [None],  # No class weight balancing\n",
    "    'max_depth': [3, 5, 7],  # Example of tuning max_depth\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Learning rate\n",
    "    'n_estimators': [100, 200],  # Number of estimators\n",
    "    'subsample': [0.8, 1.0],  # Fraction of samples used for each tree\n",
    "    'colsample_bytree': [0.8, 1.0]  # Fraction of features used for each tree\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb.LGBMClassifier(objective='binary', metric='binary_error', random_state=42),\n",
    "    param_grid=params,\n",
    "    scoring=make_scorer(roc_auc_score),  # Scoring by ROC AUC\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model with cross-validation and grid search\n",
    "grid_search.fit(train_df, y_train)\n",
    "\n",
    "# Best parameters and best score from the grid search\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best ROC-AUC score:\", grid_search.best_score_)\n",
    "\n",
    "# Step 4: Evaluate the best model on validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_best = best_model.predict(val_df)\n",
    "y_pred_prob_best = best_model.predict_proba(val_df)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Classification Report for Best Model:\")\n",
    "print(classification_report(y_valid, y_pred_best))\n",
    "print(\"ROC-AUC Score for Best Model:\", roc_auc_score(y_valid, y_pred_prob_best))\n",
    "print(\"Confusion Matrix for Best Model:\")\n",
    "print(confusion_matrix(y_valid, y_pred_best))\n",
    "\n",
    "# Step 5: Feature Importance using Gain\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": train_df.columns,\n",
    "    \"Gain\": best_model.booster_.feature_importance(importance_type=\"gain\")\n",
    "})\n",
    "\n",
    "# Sort by Gain\n",
    "importance_df = importance_df.sort_values(by=\"Gain\", ascending=False)\n",
    "\n",
    "# Print feature importance\n",
    "print(\"\\nFeature Importance (Gain):\")\n",
    "print(importance_df)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Gain\"], color=\"lightblue\")\n",
    "plt.xlabel(\"Gain\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance (Gain) in LightGBM\")\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Compute class weights\n",
    "classes = np.array([0, 1])\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "# Step 2: Train a basic RandomForest model with default settings\n",
    "rf_model = RandomForestClassifier(\n",
    "    class_weight='balanced',  # Automatically balance class weights\n",
    "    n_estimators=100,  # Number of trees in the forest\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Hyperparameter tuning with cross-validation\n",
    "params = {\n",
    "    'class_weight': ['balanced', None],  # Balance class weights automatically or leave it unbalanced\n",
    "    'max_depth': [3, 5, 7, 10],  # Max depth of trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
    "    'n_estimators': [100, 200],  # Number of trees in the forest\n",
    "    'max_features': ['sqrt', 'log2'],  # Number of features to consider for the best split\n",
    "    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=params,\n",
    "    scoring=make_scorer(roc_auc_score),  # Scoring by ROC AUC\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model with cross-validation and grid search\n",
    "grid_search.fit(train_df, y_train)\n",
    "\n",
    "# Best parameters and best score from the grid search\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best ROC-AUC score:\", grid_search.best_score_)\n",
    "\n",
    "# Step 4: Evaluate the best model on validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_best = best_model.predict(val_df)\n",
    "y_pred_prob_best = best_model.predict_proba(val_df)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Classification Report for Best Model:\")\n",
    "print(classification_report(y_valid, y_pred_best))\n",
    "\n",
    "print(\"ROC-AUC Score for Best Model:\", roc_auc_score(y_valid, y_pred_prob_best))\n",
    "\n",
    "# Confusion Matrix for Best Model\n",
    "print(\"Confusion Matrix for Best Model:\")\n",
    "print(confusion_matrix(y_valid, y_pred_best))\n",
    "\n",
    "# Step 5: Plot Feature Importance (using Gini Importance)\n",
    "importances = best_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print out the feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for f in range(len(train_df.columns)):\n",
    "    print(f\"{train_df.columns[indices[f]]}: {importances[indices[f]]}\")\n",
    "\n",
    "# Step 6: Plotting feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.barh(range(len(train_df.columns)), importances[indices], align=\"center\")\n",
    "plt.yticks(range(len(train_df.columns)), [train_df.columns[i] for i in indices])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Feature Importance (Gini)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "### For balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, precision_recall_curve, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Apply SMOTE (Oversampling minority class)\n",
    "smote = SMOTE(random_state=42)\n",
    "train_X_resampled, train_y_resampled = smote.fit_resample(train_df, y_train)\n",
    "\n",
    "print(\"Original class distribution:\", np.bincount(y_train))\n",
    "print(\"Resampled class distribution:\", np.bincount(train_y_resampled))\n",
    "\n",
    "# Train a Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_clf.fit(train_X_resampled, train_y_resampled)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gb_clf.predict(val_df)\n",
    "y_proba = gb_clf.predict_proba(val_df)[:, 1]  # Probability scores for ROC-AUC\n",
    "\n",
    "# Compute Evaluation Metrics\n",
    "def evaluate_model(model_name, y_true, y_pred, y_proba):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_proba)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)  # Confusion Matrix\n",
    "\n",
    "    print(f\"\\n{model_name} Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Evaluate Gradient Boosting Model\n",
    "evaluate_model(\"Gradient Boosting (SMOTE)\", y_valid, y_pred, y_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train a Gradient Boosting Classifier (without SMOTE)\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_clf.fit(train_df, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gb_clf.predict(val_df)\n",
    "y_proba = gb_clf.predict_proba(val_df)[:, 1]  # Probability scores for ROC-AUC\n",
    "\n",
    "# Compute Evaluation Metrics\n",
    "def evaluate_model(model_name, y_true, y_pred, y_proba):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_proba)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)  # Confusion Matrix\n",
    "\n",
    "    print(f\"\\nðŸ”¹ {model_name} Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Evaluate Gradient Boosting Model (Without SMOTE)\n",
    "evaluate_model(\"Gradient Boosting (No SMOTE)\", y_valid, y_pred, y_proba)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
