{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to train different ML models and get the best one based on the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "\n",
    "#Defining BASE_PATH\n",
    "BASE_PATH = os.getenv(\"/Users/carlos/Desktop/CURSOS/Anyone AI/Credit-risk-analysis/dataset/\", \"/Users/carlos/Desktop/CURSOS/Anyone AI/Credit-risk-analysis/dataset/\")\n",
    "\n",
    "train_file_path = os.path.join(BASE_PATH, \"X_train_data.csv\")\n",
    "y_train_file_path = os.path.join(BASE_PATH, \"y_train_data.csv\")\n",
    "val_file_path = os.path.join(BASE_PATH, \"X_val_data.csv\")\n",
    "y_val_path = os.path.join(BASE_PATH, \"y_val_data.csv\")\n",
    "\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "\n",
    "y_train_df = pd.read_csv(y_train_file_path)\n",
    "y_train = y_train_df['TARGET_LABEL_BAD']\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "val_df = pd.read_csv(val_file_path)\n",
    "\n",
    "y_valid_df = pd.read_csv(y_val_path)\n",
    "y_valid = y_valid_df['TARGET_LABEL_BAD']\n",
    "y_valid = y_valid.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [12:35:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10377, number of negative: 29623\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1679\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 89\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259425 -> initscore=-1.048959\n",
      "[LightGBM] [Info] Start training from score -1.048959\n",
      "                 Model  Accuracy  Precision    Recall  F1-score   ROC-AUC\n",
      "4              XGBoost    0.7293   0.466872  0.113739  0.182916  0.639892\n",
      "3   MLP Neural Network    0.7017   0.319774  0.106231  0.159482  0.587311\n",
      "5             LightGBM    0.7354   0.561644  0.030781  0.058363  0.646520\n",
      "1        Random Forest    0.7317   0.440252  0.026276  0.049593  0.630287\n",
      "0  Logistic Regression    0.7330   0.416667  0.005631  0.011111  0.610479\n",
      "2                  SVM    0.7335   0.000000  0.000000  0.000000  0.536479\n"
     ]
    }
   ],
   "source": [
    "#Training a set of common ML models used for binary classification looking for the best option\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(solver='liblinear'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(probability=True, kernel='rbf'),\n",
    "    \"MLP Neural Network\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def evaluate_models(models, X_train, y_train, X_val, y_val):\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_proba = model.predict_proba(X_val)[:, 1]  # Probability for class 1 (BAD)\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        roc_auc = roc_auc_score(y_val, y_proba)\n",
    "        \n",
    "        # Store results\n",
    "        results.append([name, acc, precision, recall, f1, roc_auc])\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"ROC-AUC\"])\n",
    "    return results_df\n",
    "\n",
    "# Evaluate models\n",
    "results_df = evaluate_models(models, train_df, y_train, val_df, y_valid)\n",
    "\n",
    "# Display results sorted by F1-score or ROC-AUC\n",
    "print(results_df.sort_values(by=\"F1-score\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Compute class weights\n",
    "classes = np.array([0, 1])\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "# Step 2: Train a basic RandomForest model with default settings\n",
    "rf_model = RandomForestClassifier(\n",
    "    class_weight='balanced',  # Automatically balance class weights\n",
    "    n_estimators=100,  # Number of trees in the forest\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Hyperparameter tuning with cross-validation\n",
    "params = {\n",
    "    'class_weight': ['balanced', None],  # Balance class weights automatically or leave it unbalanced\n",
    "    'max_depth': [3, 5, 7, 10],  # Max depth of trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
    "    'n_estimators': [100, 200],  # Number of trees in the forest\n",
    "    'max_features': ['sqrt', 'log2'],  # Number of features to consider for the best split\n",
    "    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=params,\n",
    "    scoring=make_scorer(roc_auc_score),  # Scoring by ROC AUC\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model with cross-validation and grid search\n",
    "grid_search.fit(train_df, y_train)\n",
    "\n",
    "# Best parameters and best score from the grid search\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best ROC-AUC score:\", grid_search.best_score_)\n",
    "\n",
    "# Step 4: Evaluate the best model on validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_best = best_model.predict(val_df)\n",
    "y_pred_prob_best = best_model.predict_proba(val_df)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Classification Report for Best Model:\")\n",
    "print(classification_report(y_valid, y_pred_best))\n",
    "\n",
    "print(\"ROC-AUC Score for Best Model:\", roc_auc_score(y_valid, y_pred_prob_best))\n",
    "\n",
    "# Confusion Matrix for Best Model\n",
    "print(\"Confusion Matrix for Best Model:\")\n",
    "print(confusion_matrix(y_valid, y_pred_best))\n",
    "\n",
    "# Step 5: Plot Feature Importance (using Gini Importance)\n",
    "importances = best_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print out the feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for f in range(len(train_df.columns)):\n",
    "    print(f\"{train_df.columns[indices[f]]}: {importances[indices[f]]}\")\n",
    "\n",
    "# Step 6: Plotting feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.barh(range(len(train_df.columns)), importances[indices], align=\"center\")\n",
    "plt.yticks(range(len(train_df.columns)), [train_df.columns[i] for i in indices])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Feature Importance (Gini)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "### For balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, precision_recall_curve, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Apply SMOTE (Oversampling minority class)\n",
    "smote = SMOTE(random_state=42)\n",
    "train_X_resampled, train_y_resampled = smote.fit_resample(train_df, y_train)\n",
    "\n",
    "print(\"Original class distribution:\", np.bincount(y_train))\n",
    "print(\"Resampled class distribution:\", np.bincount(train_y_resampled))\n",
    "\n",
    "# Train a Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_clf.fit(train_X_resampled, train_y_resampled)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gb_clf.predict(val_df)\n",
    "y_proba = gb_clf.predict_proba(val_df)[:, 1]  # Probability scores for ROC-AUC\n",
    "\n",
    "# Compute Evaluation Metrics\n",
    "def evaluate_model(model_name, y_true, y_pred, y_proba):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_proba)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)  # Confusion Matrix\n",
    "\n",
    "    print(f\"\\n{model_name} Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Evaluate Gradient Boosting Model\n",
    "evaluate_model(\"Gradient Boosting (SMOTE)\", y_valid, y_pred, y_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train a Gradient Boosting Classifier (without SMOTE)\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_clf.fit(train_df, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gb_clf.predict(val_df)\n",
    "y_proba = gb_clf.predict_proba(val_df)[:, 1]  # Probability scores for ROC-AUC\n",
    "\n",
    "# Compute Evaluation Metrics\n",
    "def evaluate_model(model_name, y_true, y_pred, y_proba):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_proba)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)  # Confusion Matrix\n",
    "\n",
    "    print(f\"\\nðŸ”¹ {model_name} Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Evaluate Gradient Boosting Model (Without SMOTE)\n",
    "evaluate_model(\"Gradient Boosting (No SMOTE)\", y_valid, y_pred, y_proba)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
